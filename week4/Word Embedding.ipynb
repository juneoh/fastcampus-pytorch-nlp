{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "\n",
    "\n",
    "![](http://i.imgur.com/agTBWiT.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (2013)\n",
    "\n",
    "[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "by Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean\n",
    "\n",
    "![](https://ascelibrary.org/cms/attachment/83d45b70-be2d-4dae-a37a-e3b51af0b7c4/figure3.jpg)\n",
    "\n",
    "* CBOW: guessing the blank\n",
    "* Skip-gram: guessing the neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NANO_CORPUS = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process', 'computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers', 'as', 'they', 'evolve', 'processes', 'manipulate', 'other', 'abstract', 'things', 'called', 'data', 'the', 'evolution', 'of', 'a', 'process', 'is', 'directed', 'by', 'a', 'pattern', 'of', 'rules', 'called', 'a', 'program', 'people', 'create', 'programs', 'to', 'direct', 'processes', 'in', 'effect', 'we', 'conjure', 'the', 'spirits', 'of', 'the', 'computer', 'with', 'our', 'spells']\n"
     ]
    }
   ],
   "source": [
    "corpus = NANO_CORPUS.lower().replace(',', ' ').replace('.', ' ').split()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'rules',\n",
       " 'conjure',\n",
       " 'data',\n",
       " 'things',\n",
       " 'pattern',\n",
       " 'called',\n",
       " 'computers',\n",
       " 'program',\n",
       " 'beings',\n",
       " 'effect',\n",
       " 'is',\n",
       " 'with',\n",
       " 'about',\n",
       " 'are',\n",
       " 'other',\n",
       " 'by',\n",
       " 'the',\n",
       " 'idea',\n",
       " 'direct',\n",
       " 'spells',\n",
       " 'process',\n",
       " 'inhabit',\n",
       " 'computational',\n",
       " 'evolve',\n",
       " 'our',\n",
       " 'as',\n",
       " 'to',\n",
       " 'programs',\n",
       " 'manipulate',\n",
       " 'of',\n",
       " 'study',\n",
       " 'create',\n",
       " 'in',\n",
       " 'abstract',\n",
       " 'spirits',\n",
       " 'directed',\n",
       " 'computer',\n",
       " 'we',\n",
       " 'they',\n",
       " 'people',\n",
       " 'evolution',\n",
       " 'processes',\n",
       " 'that']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "vocabulary = list(set(corpus))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin anything, we need to create a one-hot vector of the words. Pandas is great at this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    1\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    0\n",
       "19    0\n",
       "20    0\n",
       "21    0\n",
       "22    0\n",
       "23    0\n",
       "24    0\n",
       "25    0\n",
       "26    0\n",
       "27    0\n",
       "28    0\n",
       "29    0\n",
       "30    0\n",
       "31    0\n",
       "32    0\n",
       "33    0\n",
       "34    0\n",
       "35    0\n",
       "36    0\n",
       "37    0\n",
       "38    0\n",
       "39    0\n",
       "40    0\n",
       "41    0\n",
       "42    0\n",
       "43    0\n",
       "Name: about, dtype: uint8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = pd.get_dummies(vocabulary)\n",
    "one_hot['about']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Bag-Of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.embeddings = torch.FloatTensor(len(vocabulary), EMBEDDING_SIZE).normal_()\n",
    "        self.linear1 = torch.nn.Linear(EMBEDDING_SIZE, 128)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(128, len(vocabulary))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sum(self.embeddings * x.sum(dim=0).view(-1, 1), dim=0) # (4, 44) -> (1, 44) -> (44, 1) -> (128)\n",
    "        x = self.linear1(x) # (128)\n",
    "        x = self.relu1(x) # (128)\n",
    "        x = self.linear2(x) # (44)\n",
    "        \n",
    "        return x.view(1, -1) # (1,44)\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        return self.embeddings[vocabulary.index(word)].view(1, -1)\n",
    "\n",
    "cbow = CBOW()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 3.8296\n",
      "Epoch 1: loss 3.7166\n",
      "Epoch 2: loss 3.6075\n",
      "Epoch 3: loss 3.5016\n",
      "Epoch 4: loss 3.3993\n",
      "Epoch 5: loss 3.2989\n",
      "Epoch 6: loss 3.2000\n",
      "Epoch 7: loss 3.1023\n",
      "Epoch 8: loss 3.0059\n",
      "Epoch 9: loss 2.9115\n",
      "Epoch 10: loss 2.8184\n",
      "Epoch 11: loss 2.7272\n",
      "Epoch 12: loss 2.6369\n",
      "Epoch 13: loss 2.5483\n",
      "Epoch 14: loss 2.4611\n",
      "Epoch 15: loss 2.3758\n",
      "Epoch 16: loss 2.2919\n",
      "Epoch 17: loss 2.2095\n",
      "Epoch 18: loss 2.1286\n",
      "Epoch 19: loss 2.0491\n",
      "Epoch 20: loss 1.9715\n",
      "Epoch 21: loss 1.8952\n",
      "Epoch 22: loss 1.8203\n",
      "Epoch 23: loss 1.7476\n",
      "Epoch 24: loss 1.6762\n",
      "Epoch 25: loss 1.6067\n",
      "Epoch 26: loss 1.5384\n",
      "Epoch 27: loss 1.4719\n",
      "Epoch 28: loss 1.4073\n",
      "Epoch 29: loss 1.3445\n",
      "Epoch 30: loss 1.2838\n",
      "Epoch 31: loss 1.2249\n",
      "Epoch 32: loss 1.1680\n",
      "Epoch 33: loss 1.1135\n",
      "Epoch 34: loss 1.0608\n",
      "Epoch 35: loss 1.0105\n",
      "Epoch 36: loss 0.9623\n",
      "Epoch 37: loss 0.9161\n",
      "Epoch 38: loss 0.8722\n",
      "Epoch 39: loss 0.8302\n",
      "Epoch 40: loss 0.7902\n",
      "Epoch 41: loss 0.7526\n",
      "Epoch 42: loss 0.7165\n",
      "Epoch 43: loss 0.6826\n",
      "Epoch 44: loss 0.6505\n",
      "Epoch 45: loss 0.6201\n",
      "Epoch 46: loss 0.5914\n",
      "Epoch 47: loss 0.5643\n",
      "Epoch 48: loss 0.5388\n",
      "Epoch 49: loss 0.5147\n",
      "Epoch 50: loss 0.4921\n",
      "Epoch 51: loss 0.4707\n",
      "Epoch 52: loss 0.4506\n",
      "Epoch 53: loss 0.4315\n",
      "Epoch 54: loss 0.4137\n",
      "Epoch 55: loss 0.3968\n",
      "Epoch 56: loss 0.3810\n",
      "Epoch 57: loss 0.3660\n",
      "Epoch 58: loss 0.3519\n",
      "Epoch 59: loss 0.3386\n",
      "Epoch 60: loss 0.3259\n",
      "Epoch 61: loss 0.3141\n",
      "Epoch 62: loss 0.3028\n",
      "Epoch 63: loss 0.2922\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 64\n",
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "def get_context(i, corpus):\n",
    "    context = []\n",
    "    \n",
    "    start = max(i - WINDOW_SIZE, 0)\n",
    "    end = min(i + WINDOW_SIZE, len(corpus) - 1)\n",
    "    \n",
    "    for n in range(start, end):\n",
    "        if n == i:\n",
    "            continue\n",
    "        context.append(corpus[n])\n",
    "    \n",
    "    return context\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    n_words = 0\n",
    "    acc_loss = 0\n",
    "    for i, word in enumerate(corpus):\n",
    "        context = torch.FloatTensor(\n",
    "            [one_hot[word] for word in get_context(i, corpus)])\n",
    "        target = torch.LongTensor([vocabulary.index(word)])\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            output = cbow(context) # (4, 44) -> (1, 44)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            acc_loss += float(loss)\n",
    "            n_words += 1\n",
    "\n",
    "    print(f'Epoch {epoch}: loss {acc_loss/n_words:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a legit PyTorch module, so it can be saved just like other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.isfile('cbow.pth'):\n",
    "    print('A saved checkpoint exists.')\n",
    "else:\n",
    "    torch.save(cbow.state_dict(), 'cbow.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.load_state_dict(torch.load('cbow.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember our corpus?\n",
    "\n",
    "> We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create **programs** to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\n",
    "\n",
    "Let's see if our model can guess the highlighted word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programs\n"
     ]
    }
   ],
   "source": [
    "quiz = ['people', 'create', 'to', 'direct']\n",
    "output = cbow(torch.FloatTensor([one_hot[w] for w in quiz]))\n",
    "_, i = output.max(dim=1)\n",
    "print(vocabulary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2883, -0.4969, -1.4704, -2.0681, -0.4435, -0.7227,  0.5137,  0.6285,\n",
       "         -0.8489, -0.9536, -1.4041, -0.9300, -1.3667,  0.2518,  0.2681, -0.5468,\n",
       "         -0.4464, -0.4136, -0.5547,  0.4798,  0.7436,  1.7142,  0.8034, -0.4802,\n",
       "          0.2693,  1.3984,  1.0464, -0.8054,  2.4965,  0.5192, -1.4217, -0.7273,\n",
       "         -0.1452, -0.9530,  0.4984,  2.1563, -1.8848,  0.3289,  0.0919, -1.8581,\n",
       "         -1.0333,  0.7064,  1.1205,  0.2544, -0.6153, -1.3527, -0.8679,  0.1191,\n",
       "          0.5661, -2.2058, -1.9264, -1.3815,  0.3162,  0.5200,  0.7684, -0.1095,\n",
       "          0.5124, -0.1350, -1.4202, -0.5448,  1.8300, -1.3534,  0.7497, -0.5918,\n",
       "         -0.1150, -1.0703, -0.7705,  0.4716,  0.2581,  0.3438,  0.4342,  1.8800,\n",
       "          0.0560,  0.9551,  0.5188,  1.8680,  1.2901, -1.1749, -0.2217, -1.0311,\n",
       "          0.8857, -1.1499, -0.0370,  0.5531, -0.8168,  0.1651,  0.9904,  1.3866,\n",
       "          1.4316, -0.1049,  0.2189, -1.1159, -1.2116, -3.1350, -1.2240,  0.5033,\n",
       "          0.0931,  0.3866, -0.3147,  0.6104,  0.2705,  0.6728,  0.0608, -0.1935,\n",
       "          0.6698, -0.9610, -0.2477, -0.7306,  0.2612, -0.4074, -1.2478,  0.7203,\n",
       "         -0.8055, -1.1002,  0.3319,  0.2279, -0.1454, -2.1158,  1.0680, -1.1483,\n",
       "          0.1109, -0.0016, -1.4910, -0.8871, -1.9341, -0.5106, -1.6659,  0.3806]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.get_word_embedding('programs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embeddings = torch.FloatTensor(len(vocabulary), EMBEDDING_SIZE).normal_()\n",
    "        self.linear1 = torch.nn.Linear(EMBEDDING_SIZE, 128)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(128, len(vocabulary))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings[x]\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x.view(1, -1)\n",
    "    \n",
    "    def get_word_embedding(self, word):\n",
    "        return self.embeddings[vocabulary.index(word)].view(1, -1)\n",
    "\n",
    "skipgram = Skipgram()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(skipgram.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 1.8785\n",
      "Epoch 1: loss 1.8770\n",
      "Epoch 2: loss 1.8744\n",
      "Epoch 3: loss 1.8735\n",
      "Epoch 4: loss 1.8713\n",
      "Epoch 5: loss 1.8698\n",
      "Epoch 6: loss 1.8685\n",
      "Epoch 7: loss 1.8659\n",
      "Epoch 8: loss 1.8637\n",
      "Epoch 9: loss 1.8618\n",
      "Epoch 10: loss 1.8598\n",
      "Epoch 11: loss 1.8580\n",
      "Epoch 12: loss 1.8566\n",
      "Epoch 13: loss 1.8553\n",
      "Epoch 14: loss 1.8529\n",
      "Epoch 15: loss 1.8514\n",
      "Epoch 16: loss 1.8497\n",
      "Epoch 17: loss 1.8476\n",
      "Epoch 18: loss 1.8458\n",
      "Epoch 19: loss 1.8441\n",
      "Epoch 20: loss 1.8423\n",
      "Epoch 21: loss 1.8409\n",
      "Epoch 22: loss 1.8395\n",
      "Epoch 23: loss 1.8378\n",
      "Epoch 24: loss 1.8370\n",
      "Epoch 25: loss 1.8347\n",
      "Epoch 26: loss 1.8325\n",
      "Epoch 27: loss 1.8315\n",
      "Epoch 28: loss 1.8309\n",
      "Epoch 29: loss 1.8275\n",
      "Epoch 30: loss 1.8264\n",
      "Epoch 31: loss 1.8248\n",
      "Epoch 32: loss 1.8239\n",
      "Epoch 33: loss 1.8225\n",
      "Epoch 34: loss 1.8210\n",
      "Epoch 35: loss 1.8211\n",
      "Epoch 36: loss 1.8198\n",
      "Epoch 37: loss 1.8171\n",
      "Epoch 38: loss 1.8157\n",
      "Epoch 39: loss 1.8152\n",
      "Epoch 40: loss 1.8152\n",
      "Epoch 41: loss 1.8128\n",
      "Epoch 42: loss 1.8115\n",
      "Epoch 43: loss 1.8106\n",
      "Epoch 44: loss 1.8089\n",
      "Epoch 45: loss 1.8077\n",
      "Epoch 46: loss 1.8062\n",
      "Epoch 47: loss 1.8052\n",
      "Epoch 48: loss 1.8045\n",
      "Epoch 49: loss 1.8046\n",
      "Epoch 50: loss 1.8037\n",
      "Epoch 51: loss 1.8017\n",
      "Epoch 52: loss 1.7994\n",
      "Epoch 53: loss 1.7996\n",
      "Epoch 54: loss 1.7985\n",
      "Epoch 55: loss 1.7972\n",
      "Epoch 56: loss 1.7964\n",
      "Epoch 57: loss 1.7944\n",
      "Epoch 58: loss 1.7958\n",
      "Epoch 59: loss 1.7932\n",
      "Epoch 60: loss 1.7917\n",
      "Epoch 61: loss 1.7904\n",
      "Epoch 62: loss 1.7890\n",
      "Epoch 63: loss 1.7882\n",
      "Epoch 64: loss 1.7878\n",
      "Epoch 65: loss 1.7870\n",
      "Epoch 66: loss 1.7851\n",
      "Epoch 67: loss 1.7849\n",
      "Epoch 68: loss 1.7834\n",
      "Epoch 69: loss 1.7829\n",
      "Epoch 70: loss 1.7817\n",
      "Epoch 71: loss 1.7818\n",
      "Epoch 72: loss 1.7791\n",
      "Epoch 73: loss 1.7786\n",
      "Epoch 74: loss 1.7781\n",
      "Epoch 75: loss 1.7771\n",
      "Epoch 76: loss 1.7761\n",
      "Epoch 77: loss 1.7755\n",
      "Epoch 78: loss 1.7743\n",
      "Epoch 79: loss 1.7741\n",
      "Epoch 80: loss 1.7737\n",
      "Epoch 81: loss 1.7712\n",
      "Epoch 82: loss 1.7712\n",
      "Epoch 83: loss 1.7702\n",
      "Epoch 84: loss 1.7683\n",
      "Epoch 85: loss 1.7681\n",
      "Epoch 86: loss 1.7673\n",
      "Epoch 87: loss 1.7666\n",
      "Epoch 88: loss 1.7649\n",
      "Epoch 89: loss 1.7637\n",
      "Epoch 90: loss 1.7631\n",
      "Epoch 91: loss 1.7617\n",
      "Epoch 92: loss 1.7611\n",
      "Epoch 93: loss 1.7611\n",
      "Epoch 94: loss 1.7600\n",
      "Epoch 95: loss 1.7593\n",
      "Epoch 96: loss 1.7581\n",
      "Epoch 97: loss 1.7594\n",
      "Epoch 98: loss 1.7574\n",
      "Epoch 99: loss 1.7561\n",
      "Epoch 100: loss 1.7559\n",
      "Epoch 101: loss 1.7556\n",
      "Epoch 102: loss 1.7536\n",
      "Epoch 103: loss 1.7533\n",
      "Epoch 104: loss 1.7528\n",
      "Epoch 105: loss 1.7518\n",
      "Epoch 106: loss 1.7511\n",
      "Epoch 107: loss 1.7503\n",
      "Epoch 108: loss 1.7502\n",
      "Epoch 109: loss 1.7494\n",
      "Epoch 110: loss 1.7482\n",
      "Epoch 111: loss 1.7469\n",
      "Epoch 112: loss 1.7474\n",
      "Epoch 113: loss 1.7467\n",
      "Epoch 114: loss 1.7457\n",
      "Epoch 115: loss 1.7444\n",
      "Epoch 116: loss 1.7442\n",
      "Epoch 117: loss 1.7440\n",
      "Epoch 118: loss 1.7432\n",
      "Epoch 119: loss 1.7422\n",
      "Epoch 120: loss 1.7413\n",
      "Epoch 121: loss 1.7401\n",
      "Epoch 122: loss 1.7397\n",
      "Epoch 123: loss 1.7390\n",
      "Epoch 124: loss 1.7380\n",
      "Epoch 125: loss 1.7387\n",
      "Epoch 126: loss 1.7366\n",
      "Epoch 127: loss 1.7376\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 128\n",
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_SIZE = 128\n",
    "\n",
    "def get_context(i, corpus):\n",
    "    context = []\n",
    "    \n",
    "    start = max(i - WINDOW_SIZE, 0)\n",
    "    end = min(i + WINDOW_SIZE, len(corpus) - 1)\n",
    "    \n",
    "    for n in range(start, end):\n",
    "        if n == i:\n",
    "            continue\n",
    "        context.append(corpus[n])\n",
    "    \n",
    "    return context\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    n_words = 0\n",
    "    acc_loss = 0\n",
    "    for i, word in enumerate(corpus):\n",
    "        center = vocabulary.index(word)\n",
    "\n",
    "        for word in get_context(i, corpus):\n",
    "            context = torch.LongTensor([vocabulary.index(word)])\n",
    "\n",
    "            with torch.set_grad_enabled(True):\n",
    "                output = skipgram(center)\n",
    "                loss = criterion(output, context)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                acc_loss += float(loss)\n",
    "                n_words += 1\n",
    "\n",
    "    print(f'Epoch {epoch}: loss {acc_loss/n_words:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A checkpoint already exists.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('skipgram.pth'):\n",
    "    print('A checkpoint already exists.')\n",
    "else:\n",
    "    torch.save(skipgram.state_dict(), 'skipgram.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.load_state_dict(torch.load('skipgram.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "computer     0.148953\n",
       "computers    0.023448\n",
       "programs     0.018824\n",
       "rules        0.016518\n",
       "inhabit      0.015115\n",
       "we           0.014397\n",
       "in           0.010285\n",
       "things       0.007061\n",
       "conjure      0.006120\n",
       "they         0.005403\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_similar(query, embeddings, top_k=10):\n",
    "    embeddings = embeddings.cpu()\n",
    "    query = embeddings[vocabulary.index(query)]\n",
    "    similarity = (embeddings @ query) / (embeddings.norm() * query.norm())\n",
    "    similarity = pd.Series(dict(zip(vocabulary, similarity.numpy())))\n",
    "    similarity = similarity.sort_values(ascending=False)\n",
    "    \n",
    "    return similarity[:top_k]\n",
    "\n",
    "get_similar('computer', skipgram.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe: Global Vectors for Word Representation (2014)\n",
    "\n",
    "by Jeffrey Pennington, Richard Socher, Christopher D. Manning\n",
    "\n",
    "https://www.aclweb.org/anthology/D14-1162\n",
    "\n",
    "On page 1534:\n",
    "\n",
    "> We begin with a simple example that showcases\n",
    "how certain aspects of meaning can be extracted\n",
    "directly from co-occurrence probabilities. Consider\n",
    "two words $i$ and $j$ that exhibit a particular aspect\n",
    "of interest; for concreteness, suppose we are\n",
    "interested in the concept of thermodynamic phase,\n",
    "for which we might take $i = ice$ and $j = steam$.\n",
    "The relationship of these words can be examined\n",
    "by studying the ratio of their co-occurrence probabilities\n",
    "with various probe words, $k$. For words\n",
    "$k$ related to $ice$ but not $steam$, say $k = solid$, we\n",
    "expect the ratio $Pik / Pjk$ will be large. Similarly,\n",
    "for words $k$ related to $steam$ but not $ice$, say $k =\n",
    "gas$, the ratio should be small. For words $k$ like\n",
    "$water$ or $fashion$, that are either related to both $ice$\n",
    "and $steam$, or to neither, the ratio should be close\n",
    "to one. Table 1 shows these probabilities and their\n",
    "ratios for a large corpus, and the numbers confirm\n",
    "these expectations. Compared to the raw probabilities,\n",
    "the ratio is better able to distinguish relevant\n",
    "words ($solid$ and $gas$) from irrelevant words ($water$\n",
    "and $fashion$) and it is also better able to discriminate\n",
    "between the two relevant words.\n",
    "\n",
    "$$\n",
    "\\frac{P_{solid | ice}}{P_{solid | steam}} >\n",
    "\\frac{P_{fashion | ice}}{P_{fashion | steam}} >\n",
    "\\frac{P_{gas | ice}}{P_{gas | steam}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above argument suggests that the appropriate\n",
    "starting point for word vector learning should\n",
    "be with ratios of co-occurrence probabilities rather\n",
    "than the probabilities themselves. Noting that the\n",
    "ratio $P_{ik} /P_{jk}$ depends on three words $i$, $j$, and $k$,\n",
    "the most general model takes the form,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i, w_j, \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The number of possibilities for $F$ is vast,\n",
    "but by enforcing a few desiderata we can select a\n",
    "unique choice. First, we would like $F$ to encode\n",
    "the information present the ratio $Pik / Pjk$ in the\n",
    "word vector space. Since vector spaces are inherently\n",
    "linear structures, the most natural way to do\n",
    "this is with vector differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i - w_j, \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, we note that the arguments of $F$ in Eqn. (2)\n",
    "are vectors while the right-hand side is a scalar.\n",
    "While $F$ could be taken to be a complicated function\n",
    "parameterized by, e.g., a neural network, doing\n",
    "so would obfuscate the linear structure we are\n",
    "trying to capture. To avoid this issue, we can first\n",
    "take the dot product of the arguments,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F((w_i - w_j)^T \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, note that for\n",
    "word-word co-occurrence matrices, the distinction\n",
    "between a word and a context word is arbitrary and\n",
    "that we are free to exchange the two roles. To do so\n",
    "consistently, we must not only exchange $w \\leftrightarrow \\tilde{w}$\n",
    "but also $X \\leftrightarrow X^T$. Our final model should be invariant\n",
    "under this relabeling, but Eqn. (3) is not.\n",
    "However, the symmetry can be restored in two\n",
    "steps. First, we require that $F$ be a homomorphism\n",
    "between the groups $(\\mathbb{R}, +)$ and $(\\mathbb{R}_{>0}, \\times)$, i.e.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(X-Y)=\\frac { F(X) }{ F(Y) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i^T \\tilde{w}_k - w_j^T \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i^T \\tilde{w}_k - w_j^T \\tilde{w}_k) = \\frac{F(w_i^T \\tilde{w}_k)}{F(w_j^T \\tilde{w}_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\exp(w_i^T \\tilde{w}_k - w_j^T \\tilde{w}_k) = \\frac{\\exp(w_i^T \\tilde{w}_k)}{\\exp(w_j^T \\tilde{w}_k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F = \\exp$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 1533:\n",
    "> Let the matrix\n",
    "of word-word co-occurrence counts be denoted by\n",
    "$X$, whose entries $X_{ij}$ tabulate the number of times\n",
    "word $j$ occurs in the context of word $i$. Let $X_i = \\sum_k X_{ik}$\n",
    "be the number of times any word appears\n",
    "in the context of word $i$. Finally, let\n",
    "$P_{ij} = P(j|i) = X_{ij}/X_i$be the probability that word $j$ appear in the\n",
    "context of word $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(w_i^T \\tilde{w}_k) = P_{ik} = \\frac{X_{ik}}{X_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_i^T \\tilde { w }_k =\\log { { P }_{ ik } } =\\log ({ X_{ ik })-\\log ({ X_{ i } })  }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 1535:\n",
    "> Next, we note that Eqn. (6) would exhibit the exchange\n",
    "symmetry if not for the $log(X_i)$ on the\n",
    "right-hand side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\log(X_{ik})-\\log(X_i) \\neq \\log(X_{ki})-\\log(X_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, this term is independent\n",
    "of $k$ so it can be absorbed into a bias $b_i$ for\n",
    "$w+i$. Finally, adding an additional bias $\\tilde{b}_k$ for $\\tilde{w}_k$\n",
    "restores the symmetry,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{ w }_{ i }^{ T }\\tilde { { w }_{ k } } +{ b }_{ i }+\\tilde { { b }_{ k } } =\\log ({ X_{ ik }})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary and counting co-occurrence (again)\n",
    "\n",
    "Today's dataset, an English monolingual corpus, can be found [here](https://drive.google.com/open?id=1__lK0x_k8gtyV27QZqQUGSC4jlaQAZSC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "FILE = 'ted.en.txt'\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "vocabulary = defaultdict(int)\n",
    "co_occurrence = defaultdict(int)\n",
    "\n",
    "with open(FILE) as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = sentence.split(' ')\n",
    "    for i in range(len(words)):\n",
    "        vocabulary[words[i]] += 1\n",
    "\n",
    "        for j in range(i + 1, i + WINDOW_SIZE + 1):\n",
    "            if j >= len(words):\n",
    "                break\n",
    "            keys = tuple(sorted([words[i], words[j]]))\n",
    "            co_occurrence[keys] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how much words we have gathered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77599"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some love!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'love' in vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2444"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['love']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the dictionary into a Pandas Series for convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "MIN_OCCURRENCE = 10\n",
    "\n",
    "vocabulary = pd.Series(vocabulary, dtype='uint16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the help of Pandas, let's set a minimum frequency threshold to trim the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16754"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = vocabulary[vocabulary >= MIN_OCCURRENCE]\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'love' in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_ij = np.zeros((len(vocabulary), len(vocabulary)), dtype='uint16')\n",
    "\n",
    "for (word_i, word_j), value in co_occurrence.items():\n",
    "    try:\n",
    "        i = vocabulary.index.get_loc(word_i)\n",
    "        j = vocabulary.index.get_loc(word_j)\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "    X_ij[i][j] = value\n",
    "    X_ij[j][i] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  28, 2145,    3, ...,    0,    0,    0],\n",
       "       [2145, 5716,   49, ...,    4,    0,    4],\n",
       "       [   3,   49,    2, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   0,    4,    0, ...,    0,    0,    0],\n",
       "       [   0,    0,    0, ...,    0,    0,    0],\n",
       "       [   0,    4,    0, ...,    0,    0,    0]], dtype=uint16)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{ w }_{ i }^{ T }\\tilde { { w }_{ k } } +{ b }_{ i }+\\tilde { { b }_{ k } } =\\log ({ X_{ ik }})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, loss 0.2922\n",
      "iteration 1, loss 0.1486\n",
      "iteration 2, loss 0.0908\n",
      "iteration 3, loss 0.0709\n",
      "iteration 4, loss 0.0592\n",
      "iteration 5, loss 0.0532\n",
      "iteration 6, loss 0.0490\n",
      "iteration 7, loss 0.0460\n",
      "iteration 8, loss 0.0436\n",
      "iteration 9, loss 0.0404\n",
      "iteration 10, loss 0.0402\n",
      "iteration 11, loss 0.0377\n",
      "iteration 12, loss 0.0370\n",
      "iteration 13, loss 0.0349\n",
      "iteration 14, loss 0.0342\n",
      "iteration 15, loss 0.0326\n",
      "iteration 16, loss 0.0322\n",
      "iteration 17, loss 0.0319\n",
      "iteration 18, loss 0.0311\n",
      "iteration 19, loss 0.0305\n",
      "iteration 20, loss 0.0299\n",
      "iteration 21, loss 0.0296\n",
      "iteration 22, loss 0.0290\n",
      "iteration 23, loss 0.0287\n",
      "iteration 24, loss 0.0285\n",
      "iteration 25, loss 0.0281\n",
      "iteration 26, loss 0.0278\n",
      "iteration 27, loss 0.0276\n",
      "iteration 28, loss 0.0275\n",
      "iteration 29, loss 0.0270\n",
      "iteration 30, loss 0.0270\n",
      "iteration 31, loss 0.0268\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import torch\n",
    "\n",
    "DIM = 128\n",
    "ITERATIONS = 32\n",
    "X_MAX = 100\n",
    "ALPHA = 3/4\n",
    "GPU_ID = 2\n",
    "\n",
    "n_words = X_ij.shape[0]\n",
    "\n",
    "X = torch.from_numpy(X_ij.astype('float32')).add_(1)\n",
    "w_main = torch.FloatTensor(n_words, DIM).uniform_(-0.5, 0.5)\n",
    "w_context = torch.FloatTensor(n_words, DIM).uniform_(-0.5, 0.5)\n",
    "b_main = torch.FloatTensor(n_words).uniform_(-0.5, 0.5)\n",
    "b_context = torch.FloatTensor(n_words).uniform_(-0.5, 0.5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    X = X.cuda(device=GPU_ID)\n",
    "    w_main = w_main.cuda(device=GPU_ID)\n",
    "    w_context = w_context.cuda(device=GPU_ID)\n",
    "    b_main = b_main.cuda(device=GPU_ID)\n",
    "    b_context = b_context.cuda(device=GPU_ID)\n",
    "\n",
    "X.requires_grad_(False)\n",
    "w_main.requires_grad_(True)\n",
    "w_context.requires_grad_(True)\n",
    "b_main.requires_grad_(True)\n",
    "b_context.requires_grad_(True)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam([w_main, w_context, b_main, b_context],\n",
    "                             lr=1e-3, weight_decay=1e-15)\n",
    "\n",
    "with torch.set_grad_enabled(True):\n",
    "    for iteration in range(ITERATIONS):\n",
    "        acc_loss = 0\n",
    "        for j in torch.randperm(n_words):\n",
    "            output = w_main @ w_context[j]\n",
    "            output += b_main\n",
    "            output += b_context[j]\n",
    "            \n",
    "            loss = criterion(output, X[:, j].log() + 1e-15)\n",
    "            \n",
    "            loss_weight = (X[:, j] / X_MAX) ** ALPHA\n",
    "            loss_weight[X[:, j] > X_MAX] = 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss_weight)\n",
    "            optimizer.step()\n",
    "            \n",
    "            acc_loss += float(loss.mean())\n",
    "        \n",
    "        print(f'iteration {iteration}, loss {acc_loss/n_words:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('glove.pth'):\n",
    "    print('A checkpoint already exists.')\n",
    "else:\n",
    "    torch.save([w_main, w_context, b_main, b_context], 'glove.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_main, w_context, b_main, b_context = torch.load('glove.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "son           0.013207\n",
       "wife          0.012656\n",
       "daughter      0.012159\n",
       "mom           0.011667\n",
       "college       0.011257\n",
       "brother       0.011073\n",
       "husband       0.011053\n",
       "named         0.010624\n",
       "sister        0.010571\n",
       "teacher       0.010530\n",
       "dad           0.010478\n",
       "moved         0.010311\n",
       "career        0.010187\n",
       "hospital      0.010160\n",
       "doctor        0.010071\n",
       "boy           0.009906\n",
       "colleagues    0.009823\n",
       "walked        0.009803\n",
       "month         0.009781\n",
       "sat           0.009772\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def v(word):\n",
    "    i = vocabulary.index.get_loc(word)\n",
    "    return w_main[i].cpu()\n",
    "\n",
    "def analogy(target, top_k=20):\n",
    "    target /= target.norm()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        similarity = (w_main.cpu() @ target) / (w_main.cpu().norm() * target.norm())\n",
    "        similarity = pd.Series(dict(zip(vocabulary.keys(), similarity.numpy())))\n",
    "        similarity = similarity[vocabulary < 500]\n",
    "        similarity = similarity.sort_values(ascending=False)\n",
    "    \n",
    "    return similarity.sort_values(ascending=False)[:top_k]\n",
    "\n",
    "analogy(v('wife') - v('man') + v('woman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bodies       0.011712\n",
       "develop      0.011325\n",
       "treat        0.010971\n",
       "healthy      0.010836\n",
       "skin         0.010540\n",
       "genetic      0.010443\n",
       "protect      0.010405\n",
       "materials    0.010383\n",
       "brains       0.010379\n",
       "measure      0.010312\n",
       "genes        0.010257\n",
       "neurons      0.010108\n",
       "objects      0.010095\n",
       "models       0.010018\n",
       "decisions    0.009981\n",
       "function     0.009876\n",
       "improve      0.009812\n",
       "interact     0.009690\n",
       "machines     0.009677\n",
       "plants       0.009677\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(v('robots') - v('robot') + v('body'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pytorch-v0.4.1)",
   "language": "python",
   "name": "pytorch-v0.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
